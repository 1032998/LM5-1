<p align="center">
    <br>
    <img src="./pics/banner.png" width="500"/>
    <br>
</p>
<p align="center">
    <a href="https://github.com/ymcui/MacBERT/blob/master/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/ymcui/MacBERT.svg?color=blue&style=flat-square">
    </a>
</p>

This repository contains the resources in our paper "**Revisiting Pre-trained Models for Chinese Natural Language Processing**", which will be published in "[Findings of EMNLP](https://2020.emnlp.org)". We are still finalizing our camera-ready paper. Meanwhile, you can read our pre-print version in April 2020.

**[Revisiting Pre-trained Models for Chinese Natural Language Processing](https://arxiv.org/abs/2004.13922)**  
*Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu*


For resources other than MacBERT, please visit the following repositories:
- Chinese BERT-wwm series: https://github.com/ymcui/Chinese-BERT-wwm
- Chinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA
- Chinese XLNet: https://github.com/ymcui/Chinese-XLNet

More resources by HFL: https://github.com/ymcui/HFL-Anthology


## News
**[Sep 23, 2020] Thank you for your attention and support. We are expected to release our models at the end of October (est.). In the meantime, the issue section will be closed.** 

[Sep 15, 2020] Our paper ["Revisiting Pre-Trained Models for Chinese Natural Language Processing"](https://arxiv.org/abs/2004.13922) is accepted to [Findings of EMNLP](https://2020.emnlp.org) as a long paper.


## Guide
| Section | Description |
|-|-|
| [Introduction](#Introduction) | Introduction to MacBERT |
| [Download](#Download) | Download links for MacBERT |
| [Quick Load](#Quick-Load) | Learn how to quickly load our models through [ðŸ¤—Transformers](https://github.com/huggingface/transformers) |
| [Results](#Results) | Results on several Chinese NLP datasets |
| [FAQ](#FAQ) | Frequently Asked Questions |
| [Citation](#Citation) | Citation |


## Introduction
TBA


## Download
TBA


## Quick Load
TBA


## Results
TBA

## FAQ
TBA

## Citation
If you find our resource or paper is useful, please consider including the following citation in your paper.
- https://arxiv.org/abs/2004.13922
```
@inproceedings{cui-etal-2020-revisiting,
    title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
    author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Wang, Shijin and Hu, Guoping},
    booktitle = "Findings of EMNLP",
    year = "2020",
    publisher = "Association for Computational Linguistics"
}
```

## Acknowledgement
The first author would like to thank [Google TensorFlow Research Cloud (TFRC) Program](https://www.tensorflow.org/tfrc).

## Issues
Before you submit an issue:

- **You are advised to read [FAQ](#FAQ) first before you submit an issue.** 
- Repetitive and irrelevant issues will be ignored and closed by [stable-bot](stale Â· GitHub Marketplace). Thank you for your understanding and support.
- We cannot acommodate EVERY request, and thus please bare in mind that there is no guarantee that your request will be met.
- Always be polite when you submit an issue.

